# Olumi Assistants Service - Environment Configuration
# Copy to .env and fill in actual values (never commit .env)
#
# REQUIRED variables are marked; others have sensible defaults.

# ============================================================================
# Runtime
# ============================================================================
NODE_ENV=development
PORT=3101

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Options: "fixtures" (safe default, no API key needed), "anthropic", "openai"
LLM_PROVIDER=fixtures

# LLM_MODEL - Override default model for provider (optional)
# Examples: claude-sonnet-4-20250514, gpt-4o
# LLM_MODEL=

# LLM_FAILOVER_PROVIDERS - Comma-separated fallback providers (optional)
# Example: openai,anthropic
# LLM_FAILOVER_PROVIDERS=

# ============================================================================
# API Keys
# ============================================================================
# REQUIRED (if using real LLM providers) - at least one must be set
# Leave blank when using LLM_PROVIDER=fixtures
ANTHROPIC_API_KEY=
OPENAI_API_KEY=

# ============================================================================
# Client Authentication
# ============================================================================
# REQUIRED in production - at least one must be set:
# Use ASSIST_API_KEYS (plural) for production with multiple clients (comma-separated)
# Use ASSIST_API_KEY (singular) for local dev with a single key
ASSIST_API_KEYS=
# ASSIST_API_KEY=

# HMAC Authentication (alternative to API keys)
# 64-byte hex secret for HMAC signature verification
# HMAC_SECRET=

# ============================================================================
# Safety Limits
# ============================================================================
COST_MAX_USD=1.00
BODY_LIMIT_BYTES=1048576

# ============================================================================
# Timeouts & Retries
# ============================================================================
# Central timeout configuration (src/config/timeouts.ts)
HTTP_CLIENT_TIMEOUT_MS=15000
ROUTE_TIMEOUT_MS=120000
UPSTREAM_RETRY_DELAY_MS=1000
SSE_MAX_MS=120000

# ============================================================================
# CORS Configuration
# ============================================================================
# Comma-separated list of allowed origins, or "*" for local development only
# REQUIRED in production - use explicit origins (e.g., https://olumi.app,https://app.olumi.app)
# WARNING: "*" is rejected in production for security
ALLOWED_ORIGINS=*

# ============================================================================
# Rate Limiting
# ============================================================================
# Global rate limit (per IP, all routes)
GLOBAL_RATE_LIMIT_RPM=120
# Per-key rate limit (authenticated requests)
RATE_LIMIT_RPM=120
# SSE-specific rate limit
SSE_RATE_LIMIT_RPM=20

# CEE feature-specific rate limits (optional overrides)
# CEE_DRAFT_RATE_LIMIT_RPM=5
# CEE_STREAM_RATE_LIMIT_RPM=20
# CEE_OPTIONS_RATE_LIMIT_RPM=10
# CEE_BIAS_CHECK_RATE_LIMIT_RPM=10
# CEE_EVIDENCE_HELPER_RATE_LIMIT_RPM=10
# CEE_SENSITIVITY_COACH_RATE_LIMIT_RPM=10
# CEE_TEAM_PERSPECTIVES_RATE_LIMIT_RPM=10
# CEE_EXPLAIN_RATE_LIMIT_RPM=10
# CEE_KEY_INSIGHT_RATE_LIMIT_RPM=30
# CEE_ELICIT_BELIEF_RATE_LIMIT_RPM=30
# CEE_UTILITY_WEIGHT_RATE_LIMIT_RPM=30
# CEE_RISK_TOLERANCE_RATE_LIMIT_RPM=30
# CEE_EDGE_FUNCTION_RATE_LIMIT_RPM=60
# CEE_GRAPH_READINESS_RATE_LIMIT_RPM=60

# ============================================================================
# Logging & Observability
# ============================================================================
LOG_LEVEL=info
PROMETHEUS_ENABLE=1

# Datadog Integration (optional)
# DD_AGENT_HOST=127.0.0.1
# DD_AGENT_PORT=8125
# DD_SERVICE=olumi-assistants-service
# DD_ENV=development
# DD_API_KEY=

# Performance Tracing (optional, for debugging)
# PERF_TRACE=1

# ============================================================================
# CEE Feature Flags
# ============================================================================
# Enable single-goal enforcement
CEE_ENFORCE_SINGLE_GOAL=true

# Preflight validation before LLM calls
CEE_PREFLIGHT_ENABLED=true

# Clarification enforcement (multi-turn flow)
# CEE_CLARIFICATION_ENFORCED=false

# Diagnostics endpoint (requires key ID allowlist)
CEE_DIAGNOSTICS_ENABLED=false
# CEE_DIAGNOSTICS_KEY_IDS=key-id-1,key-id-2

# Decision review example endpoint (disabled by default)
# CEE_DECISION_REVIEW_EXAMPLE_ENABLED=false

# ============================================================================
# ISL (Inference & Structure Learning) Integration
# ============================================================================
# Enable causal validation enrichment for bias detection
CEE_CAUSAL_VALIDATION_ENABLED=false
# ISL_BASE_URL=http://localhost:8888
# ISL_TIMEOUT_MS=5000
# ISL_MAX_RETRIES=1
# ISL_API_KEY=

# ============================================================================
# Share & Review Feature
# ============================================================================
# Enable share review functionality
# SHARE_REVIEW_ENABLED=false
# Secret for share token signing (64-byte hex)
# SHARE_SECRET=

# ============================================================================
# Prompt Management (v2.0)
# ============================================================================
# Enable prompt store for dynamic prompt management
# PROMPTS_ENABLED=false
# Admin API key for prompt management endpoints
# ADMIN_API_KEY=
# Braintrust integration for A/B testing
# PROMPTS_BRAINTRUST_ENABLED=false
# BRAINTRUST_API_KEY=

# ============================================================================
# PLoT Engine Integration (optional)
# ============================================================================
# Base URL for external PLoT engine
# ENGINE_BASE_URL=
