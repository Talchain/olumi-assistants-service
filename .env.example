# Olumi Assistants Service - Environment Configuration
# Copy to .env and fill in actual values (never commit .env)

# Runtime
NODE_ENV=development
PORT=3101

# LLM Provider Configuration
# Options: "fixtures" (safe default, no API key needed), "anthropic", "openai"
LLM_PROVIDER=fixtures

# API Keys (required only if using real LLM providers)
# Leave blank when using LLM_PROVIDER=fixtures
ANTHROPIC_API_KEY=
OPENAI_API_KEY=

# Client Authentication
# Use ASSIST_API_KEYS (plural) for production with multiple clients (comma-separated)
# Use ASSIST_API_KEY (singular) for local dev with a single key
ASSIST_API_KEYS=
# ASSIST_API_KEY=

# Safety Limits
COST_MAX_USD=1.00
BODY_LIMIT_BYTES=1048576

# Timeouts & Retries
# Central timeout configuration (src/config/timeouts.ts)
HTTP_CLIENT_TIMEOUT_MS=15000
ROUTE_TIMEOUT_MS=120000
UPSTREAM_RETRY_DELAY_MS=1000
SSE_MAX_MS=120000

# CORS Configuration
# Comma-separated list of allowed origins, or "*" for local development only
# Production should use explicit origins (e.g., https://olumi.app,https://app.olumi.app)
ALLOWED_ORIGINS=*

# Rate Limiting
# Global rate limit (per IP, all routes)
GLOBAL_RATE_LIMIT_RPM=120
# Per-key rate limit (authenticated requests)
RATE_LIMIT_RPM=120
# SSE-specific rate limit
SSE_RATE_LIMIT_RPM=20
# CEE feature-specific rate limits (optional overrides)
# CEE_BIAS_CHECK_RATE_LIMIT_RPM=120

# Logging
LOG_LEVEL=info

PROMETHEUS_ENABLE=1

# ISL (Inference & Structure Learning) Integration
# Enable causal validation enrichment for bias detection
CEE_CAUSAL_VALIDATION_ENABLED=false
# ISL_BASE_URL=http://localhost:8888
# ISL_TIMEOUT_MS=5000
# ISL_MAX_RETRIES=2
# ISL_API_KEY=
